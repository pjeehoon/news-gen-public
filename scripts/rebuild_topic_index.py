#!/usr/bin/env python3
"""
Í∏∞Ï°¥ ÏÉùÏÑ±Îêú Í∏∞ÏÇ¨Îì§Î°úÎ∂ÄÌÑ∞ topic_index.jsonÏùÑ Ïû¨Íµ¨ÏÑ±ÌïòÎäî Ïä§ÌÅ¨Î¶ΩÌä∏
GitHub ActionsÏóêÏÑú Ï§ëÎ≥µ Ï≤¥ÌÅ¨Î•º ÏúÑÌï¥ ÏÇ¨Ïö©Îê®
"""

import os
import json
import re
from datetime import datetime
from pathlib import Path
from bs4 import BeautifulSoup
import logging

# Î°úÍπÖ ÏÑ§Ï†ï
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def extract_metadata_from_html(html_path: str) -> dict:
    """HTML ÌååÏùºÏóêÏÑú Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú"""
    try:
        with open(html_path, 'r', encoding='utf-8') as f:
            soup = BeautifulSoup(f.read(), 'html.parser')
        
        metadata = {}
        
        # Ï†úÎ™© Ï∂îÏ∂ú
        title_tag = soup.find('title')
        if title_tag:
            metadata['title'] = title_tag.text.strip()
        
        # article-data script ÌÉúÍ∑∏ÏóêÏÑú JSON Îç∞Ïù¥ÌÑ∞ Ï∂îÏ∂ú
        article_data_script = soup.find('script', id='article-data')
        if article_data_script and article_data_script.string:
            try:
                article_data = json.loads(article_data_script.string)
                metadata.update(article_data)
            except json.JSONDecodeError:
                logger.warning(f"Failed to parse article data from {html_path}")
        
        # data-* ÏÜçÏÑ±ÏóêÏÑú Ï∂îÍ∞Ä Ï†ïÎ≥¥ Ï∂îÏ∂ú
        article_link = soup.find('a', class_='article-link')
        if article_link:
            for attr, value in article_link.attrs.items():
                if attr.startswith('data-'):
                    key = attr.replace('data-', '').replace('-', '_')
                    metadata[key] = value
        
        # ÎÇ†Ïßú Ï∂îÏ∂ú
        date_element = soup.find('span', class_='date')
        if date_element:
            metadata['date'] = date_element.text.strip()
        
        # ÌååÏùºÎ™ÖÏóêÏÑú topic_id Ï∂îÏ∂ú
        filename = os.path.basename(html_path)
        match = re.search(r'article_(\d{8}_\d{6})', filename)
        if match:
            metadata['file_id'] = match.group(1)
        
        return metadata
        
    except Exception as e:
        logger.error(f"Error extracting metadata from {html_path}: {e}")
        return {}

def rebuild_topic_index():
    """topic_index.json Ïû¨Íµ¨ÏÑ±"""
    
    # Ï∫êÏãú ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±
    cache_dir = Path("cache/articles")
    cache_dir.mkdir(parents=True, exist_ok=True)
    
    # Í∏∞Ï°¥ topic_index.jsonÏù¥ ÏûàÏúºÎ©¥ Î°úÎìú
    topic_index_path = cache_dir / "topic_index.json"
    if topic_index_path.exists():
        try:
            with open(topic_index_path, 'r', encoding='utf-8') as f:
                topic_index = json.load(f)
                logger.info(f"Loaded existing topic index with {len(topic_index)} entries")
        except Exception as e:
            logger.warning(f"Failed to load existing topic index: {e}")
            topic_index = {}
    else:
        topic_index = {}
    
    processed_count = 0
    
    # smart_articles ÎîîÎ†âÌÜ†Î¶¨Ïùò HTML ÌååÏùºÎì§ Ï≤òÎ¶¨
    smart_articles_dir = Path("output/smart_articles")
    if smart_articles_dir.exists():
        # Î™®Îì† HTML ÌååÏùº Ï∞æÍ∏∞ (article_*.html Ìå®ÌÑ¥ÎøêÎßå ÏïÑÎãàÎùº)
        html_files = list(smart_articles_dir.glob("*.html"))
        logger.info(f"Found {len(html_files)} HTML files in {smart_articles_dir}")
        
        for html_file in html_files:
            # about.html, index.html Îì±ÏùÄ Ï†úÏô∏
            if html_file.name in ['index.html', 'about.html', 'admin.html']:
                continue
                
            metadata = extract_metadata_from_html(str(html_file))
            
            if metadata.get('topic_id'):
                topic_id = metadata['topic_id']
                
                # topic_index Ìï≠Î™© Íµ¨ÏÑ±
                topic_entry = {
                    'main_title': metadata.get('title', ''),
                    'generated_title': metadata.get('generated_title', metadata.get('title', '')),
                    'keywords': metadata.get('keywords', []),
                    'created_at': metadata.get('created_at', datetime.now().isoformat()),
                    'last_updated': metadata.get('last_updated', datetime.now().isoformat()),
                    'version': metadata.get('version', 1),
                    'tags': metadata.get('tags', {
                        'category_tags': [],
                        'content_tags': []
                    }),
                    'source_articles': metadata.get('source_articles', [])
                }
                
                # version_historyÍ∞Ä ÏûàÏúºÎ©¥ Ï∂îÍ∞Ä
                if 'version_history' in metadata:
                    topic_entry['version_history'] = metadata['version_history']
                
                topic_index[topic_id] = topic_entry
                processed_count += 1
                logger.info(f"Processed: {topic_id} - {topic_entry['main_title']}")
    
    # topic_index.json Ï†ÄÏû•
    topic_index_path = cache_dir / "topic_index.json"
    with open(topic_index_path, 'w', encoding='utf-8') as f:
        json.dump(topic_index, f, ensure_ascii=False, indent=2)
    
    logger.info(f"‚úÖ Topic index rebuilt successfully")
    logger.info(f"   Total articles indexed: {processed_count}")
    logger.info(f"   Index saved to: {topic_index_path}")
    
    # ÏöîÏïΩ Ï∂úÎ†•
    if processed_count > 0:
        print(f"\nüìä Index Summary:")
        print(f"   - Total topics: {len(topic_index)}")
        print(f"   - Latest article: {max((entry['created_at'] for entry in topic_index.values()), default='N/A')}")
        print(f"   - Index location: {topic_index_path}")
    else:
        print("\n‚ö†Ô∏è  No existing articles found to index")
        print("   This is normal for the first run or after cleaning the project")

if __name__ == "__main__":
    rebuild_topic_index()